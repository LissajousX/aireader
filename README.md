<div align="center">

<img src="screenshots/icon.png" width="80" alt="Aireader" />

# Aireader

### Your Documents. Your GPU. Your Knowledge.

**AI-powered reading assistant that runs 100% on your machine**

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Tauri 2.0](https://img.shields.io/badge/Tauri-2.0-orange)](https://tauri.app)
[![llama.cpp](https://img.shields.io/badge/llama.cpp-b7966-green)](https://github.com/ggerganov/llama.cpp)
[![Platform](https://img.shields.io/badge/Platform-Windows%20Â·%20macOS%20Â·%20Ubuntu-lightgrey)]()

**[English](README_EN.md)** Â· **[ä¸­æ–‡](README_CN.md)** Â· **[User Guide](USER_GUIDE_EN.md)** Â· **[ç”¨æˆ·æŒ‡å—](USER_GUIDE_CN.md)**

</div>

---

> Most AI reading tools send your documents to the cloud. **Aireader doesn't.**
>
> Built-in llama.cpp engine. AI runs on your CPU/GPU. Data never leaves your machine.

<div align="center">

![Main Interface](screenshots/main-interface.png)

</div>

## âœ¨ Highlights

- **ğŸ”’ 100% Offline** â€” Zero cloud dependency. Your documents stay on your machine.
- **âš¡ Smart Hardware Matching** â€” Auto-detects GPU â†’ benchmarks all backends (CUDA / Vulkan / Metal / CPU) â†’ picks the fastest â†’ you choose the model.
- **ğŸ“– Reading-First** â€” Not a chatbot. A real AI assistant for deep reading: select text â†’ translate â†’ explain â†’ save notes.
- **ğŸŒ Flexible AI Backends** â€” Built-in Qwen3 (0.6Bâ€“32B), or connect Ollama / OpenAI-compatible APIs.
- **ğŸ“š Offline Dictionaries** â€” Built-in ECDICT + CC-CEDICT. Double-click any word.

## ï¿½ï¸ Supported Platforms

| Platform | GPU Acceleration |
|:---|:---|
| **Windows x64** | CUDA 12.4/13.1 Â· Vulkan Â· CPU |
| **macOS arm64** | Metal (CPU+GPU unified) |
| **macOS x64** | CPU |
| **Ubuntu x64** | Vulkan Â· CPU |

## ğŸš€ Quick Start

```bash
npm install          # Install dependencies
npm run tauri dev    # Development mode
npm run tauri build  # Build for production
```

**First launch** â†’ Setup wizard â†’ Language â†’ Storage paths â†’ Multi-engine benchmark â†’ Choose model â†’ Start reading.

## ğŸ—ï¸ Tech Stack

| Layer | Technology |
|:---|:---|
| Frontend | React 18 Â· TypeScript Â· TailwindCSS Â· Zustand |
| Desktop | Tauri 2.0 (Rust) |
| AI Engine | llama.cpp b7966 Â· llama-bench Â· Qwen3 0.6Bâ€“32B (Q4_K_M) |
| Rendering | react-pdf / pdf.js Â· epub.js Â· react-markdown |
| Storage | SQLite (rusqlite) Â· localStorage |
| Dictionary | [ECDICT](https://github.com/skywind3000/ECDICT) Â· [CC-CEDICT](https://cc-cedict.org/) |

<details>
<summary><b>ğŸ“ Project Structure</b></summary>

```
aireader/
â”œâ”€â”€ src/                        # React frontend
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ai/                 # AI panel & contextual chat
â”‚   â”‚   â”œâ”€â”€ help/               # Help modal
â”‚   â”‚   â”œâ”€â”€ layout/             # Sidebar, welcome, document library
â”‚   â”‚   â”œâ”€â”€ notes/              # Notes panel
â”‚   â”‚   â”œâ”€â”€ reader/             # PDF / EPUB / TXT / MD readers
â”‚   â”‚   â”œâ”€â”€ settings/           # Settings modal
â”‚   â”‚   â”œâ”€â”€ setup/              # First-launch setup wizard
â”‚   â”‚   â””â”€â”€ ui/                 # Shared UI & dictionary popup
â”‚   â”œâ”€â”€ config/                 # Download URLs & model tiers
â”‚   â”œâ”€â”€ i18n/                   # Internationalization (CN/EN)
â”‚   â”œâ”€â”€ services/               # Ollama API & streaming
â”‚   â”œâ”€â”€ stores/                 # Zustand state management
â”‚   â””â”€â”€ types/                  # TypeScript type definitions
â”œâ”€â”€ src-tauri/                  # Rust backend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs              # Tauri commands & app config
â”‚   â”‚   â”œâ”€â”€ builtin_llm.rs      # llama.cpp integration & model management
â”‚   â”‚   â”œâ”€â”€ database.rs         # SQLite note storage
â”‚   â”‚   â”œâ”€â”€ dictionary.rs       # ECDICT / CC-CEDICT dictionary
â”‚   â”‚   â”œâ”€â”€ epub.rs             # EPUB extraction
â”‚   â”‚   â””â”€â”€ ollama.rs           # Ollama proxy
â”‚   â”œâ”€â”€ resources/              # Dictionaries & sample documents
â”‚   â””â”€â”€ Cargo.toml
â””â”€â”€ package.json
```

</details>

## ğŸ™ Acknowledgments

- [llama.cpp](https://github.com/ggerganov/llama.cpp) â€” Local LLM inference engine
- [Tauri](https://tauri.app) â€” Desktop application framework
- [Qwen3](https://github.com/QwenLM/Qwen3) â€” Built-in language models
- [ECDICT](https://github.com/skywind3000/ECDICT) â€” Offline English-Chinese dictionary
- [CC-CEDICT](https://cc-cedict.org/) â€” Offline Chinese-English dictionary

## ğŸ“„ License

[MIT](LICENSE) Â© xujiayu
